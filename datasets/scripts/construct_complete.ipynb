{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import sqlparse\n",
    "from sqlparse import sql\n",
    "import sqlglot\n",
    "import sqlglot.expressions as exp\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from process_sql import create_sql, SQLParseException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading whole english spider dataset for further tests\n",
    "spider_en = []\n",
    "with open('../../spider-en/train_spider.json') as json_data:\n",
    "    spider_en.extend(json.load(json_data))\n",
    "with open('../../spider-en/train_others.json') as json_data:\n",
    "    spider_en.extend(json.load(json_data))\n",
    "with open('../../spider-en/dev.json') as json_data:\n",
    "    spider_en.extend(json.load(json_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query toks no value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_query_no_value(query):\n",
    "    statement = sqlparse.parse(query)[0]\n",
    "    \n",
    "    value_tokens = [token for token in statement.flatten() if str(token.ttype).startswith('Token.Literal')]\n",
    "    for token in value_tokens:\n",
    "        token.value = 'value'\n",
    "        \n",
    "    coarse_tokens =  [str(token).lower() for token in statement.flatten() if str(token).strip() != '']\n",
    "    \n",
    "    fine_tokens = []\n",
    "    for token in coarse_tokens:\n",
    "        if len(token.split(' ')) > 1:\n",
    "            fine_tokens.extend(token.split(' '))\n",
    "        elif token == '!=':\n",
    "            fine_tokens.extend(['!', '='])\n",
    "        elif token == '>=':\n",
    "            fine_tokens.extend(['>', '='])\n",
    "        elif token == '<=':\n",
    "            fine_tokens.extend(['<', '='])\n",
    "        elif token == ';':\n",
    "            continue\n",
    "        else:\n",
    "            fine_tokens.append(token)\n",
    "    return fine_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare created tokenization function with tokens from oryginal spider\n",
    "discrepancies = 0\n",
    "\n",
    "for sample in spider_en:\n",
    "    my_tokens = tokenize_query_no_value(sample['query'])\n",
    "    oryginal_tokens = sample['query_toks_no_value']\n",
    "    if my_tokens != oryginal_tokens:\n",
    "        discrepancies += 1\n",
    "        print(sample['query'])\n",
    "        print(my_tokens)\n",
    "        print(oryginal_tokens)\n",
    "        print()\n",
    "        \n",
    "print(discrepancies, 'discrepancies')\n",
    "# found 18 discrepancies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pl = spacy.load(\"pl_core_news_md\")\n",
    "\n",
    "def tokenize_polish(text):\n",
    "    return [str(token) for token in nlp_pl(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_statement(root, tokens=None):\n",
    "    if tokens is None:\n",
    "        tokens = []\n",
    "    \n",
    "    # Create tokens from elements which we can't divide\n",
    "    if not hasattr(root, 'tokens'):\n",
    "        token = str(root).strip()\n",
    "        if token == '' or token == ';':\n",
    "            pass\n",
    "        elif len(token.split(' ')) > 1:\n",
    "            tokens.extend(token.split(' '))\n",
    "        elif token == '!=':\n",
    "            tokens.extend(['!', '='])\n",
    "        elif token == '>=':\n",
    "            tokens.extend(['>', '='])\n",
    "        elif token == '<=':\n",
    "            tokens.extend(['<', '='])\n",
    "        else:\n",
    "            tokens.append(str(root))\n",
    "    \n",
    "    # Not split identifiers like \"T1.name\" into separate tokens\n",
    "    elif isinstance(root, sql.Identifier) and '.' in str(root):\n",
    "        tokens.append(str(root))\n",
    "        \n",
    "    # Tokenize strings using polish tokenizer\n",
    "    elif isinstance(root, sql.Identifier) and len(root.tokens) == 1 and str(root.tokens[0].ttype) == 'Token.Literal.String.Symbol':\n",
    "        tokens.extend(tokenize_polish(str(root)))\n",
    "        \n",
    "    # Tokenize other compound elements recursively\n",
    "    else:\n",
    "        for token in root.tokens:\n",
    "            tokenize_statement(token, tokens)\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_query(query):\n",
    "    statement = sqlparse.parse(query)[0]\n",
    "    tokens = tokenize_statement(statement)\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare created tokenization function with tokens from oryginal spider\n",
    "discrepancies = 0\n",
    "\n",
    "for sample in spider_en:\n",
    "    my_tokens = tokenize_query(sample['query'])\n",
    "    oryginal_tokens = sample['query_toks']\n",
    "    if my_tokens != oryginal_tokens:\n",
    "        discrepancies += 1\n",
    "        print(sample['query'])\n",
    "        print(my_tokens)\n",
    "        print(oryginal_tokens)\n",
    "        print()\n",
    "        \n",
    "print(discrepancies, 'discrepancies')\n",
    "# found 4754 discrepancies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_question(question):\n",
    "    return tokenize_polish(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_calculated_attributes_to_samples(src_path, tgt_path, tables_path):\n",
    "    with open(src_path) as f:\n",
    "        samples = json.load(f)\n",
    "        \n",
    "    new_samples = []\n",
    "        \n",
    "    for sample in tqdm(samples):\n",
    "        try:\n",
    "            sql = create_sql(sample['db_id'], sample['query_pl'], tables_path)\n",
    "        except SQLParseException:\n",
    "            # skip samples with invalid sqls\n",
    "            print('Skipping sample')\n",
    "        \n",
    "        new_sample = {\n",
    "            'db_id': sample['db_id'],\n",
    "            'question': sample['question_pl'],\n",
    "            'question_toks': tokenize_question(sample['question_pl']),\n",
    "            'query': sample['query_pl'],\n",
    "            'query_toks': tokenize_query(sample['query_pl']),\n",
    "            'query_toks_no_value': tokenize_query_no_value(sample['query_pl']),\n",
    "            'sql': sql\n",
    "        }\n",
    "        \n",
    "        new_samples.append(new_sample)\n",
    "        \n",
    "    with open(tgt_path, 'w') as f:\n",
    "        json.dump(new_samples, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_sql(src_files, tgt_file):\n",
    "    with open(tgt_file, 'w') as f:\n",
    "        for src_file in src_files:\n",
    "            with open(src_file) as g:\n",
    "                for sample in json.load(g):\n",
    "                    f.write(f\"{sample['query_pl']}\\t{sample['db_id']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables_translation_dict(tables_names_path):\n",
    "    with open(tables_names_path) as f:\n",
    "        tables_names = json.load(f)\n",
    "        \n",
    "    tables_trans_dict = {}\n",
    "    for entry in tables_names:\n",
    "        if entry['db_id'] not in tables_trans_dict:\n",
    "            tables_trans_dict[entry['db_id']] = {}\n",
    "        tables_trans_dict[entry['db_id']][entry['name_original'].lower()] = {'name': entry['name_pl'], 'name_original': entry['name_original_pl']}\n",
    "        \n",
    "    return tables_trans_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_columns_translation_dict(columns_names_path):\n",
    "    with open(columns_names_path) as f:\n",
    "        columns_names = json.load(f)\n",
    "        \n",
    "    columns_trains_dict = {}\n",
    "    for entry in columns_names:\n",
    "        if entry['db_id'] not in columns_trains_dict:\n",
    "            columns_trains_dict[entry['db_id']] = {}\n",
    "        db_stuff = columns_trains_dict[entry['db_id']]\n",
    "        if entry['table_name_original'].lower() not in db_stuff:\n",
    "            db_stuff[entry['table_name_original'].lower()] = {}\n",
    "        table_stuff = db_stuff[entry['table_name_original'].lower()]\n",
    "        table_stuff[entry['column_name_original'].lower()] = {'name': entry['column_name_pl'], 'name_original': entry['column_name_original_pl']}\n",
    "        \n",
    "    return columns_trains_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tables(tables_path, tables_names_path, columns_names_path, output_path):\n",
    "    tables_trans_dict = create_tables_translation_dict(tables_names_path)\n",
    "    columns_trans_dict = create_columns_translation_dict(columns_names_path)\n",
    "        \n",
    "    with open(tables_path) as f:\n",
    "        tables_json = json.load(f)\n",
    "        \n",
    "    # perform translation\n",
    "    for db in tables_json:\n",
    "        db_id = db['db_id']\n",
    "        \n",
    "        # translate columns\n",
    "        assert db['column_names'][0][1] == '*'\n",
    "        assert db['column_names_original'][0][1] == '*'\n",
    "        for i in range(1, len(db['column_names_original'])):\n",
    "            table_idx, column_name_original = db['column_names_original'][i]\n",
    "            table_name = db['table_names_original'][table_idx]\n",
    "            translations = columns_trans_dict[db_id][table_name][column_name_original]\n",
    "            db['column_names_original'][i][1] = translations['name_original']\n",
    "            db['column_names'][i][1] = translations['name']\n",
    "        \n",
    "        # translate tables\n",
    "        for i in range(len(db['table_names_original'])):\n",
    "            table_name = db['table_names_original'][i]\n",
    "            translations = tables_trans_dict[db_id][table_name]\n",
    "            db['table_names_original'][i] = translations['name']\n",
    "            db['table_names'][i] = translations['name_original']\n",
    "        \n",
    "    # save translated\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(tables_json, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = translate_tables(\n",
    "    '../auxiliary/translated_schema/tables.json',\n",
    "    '../auxiliary/translated_schema/tables_names.json',\n",
    "    '../auxiliary/translated_schema/columns_names.json',\n",
    "    '../auxiliary/translated_likes/tables.json'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nazwy kolumn:\n",
    "- po select\n",
    "    - w funkcjach agregacyjnych (min, max, avg)\n",
    "- w porównaniach (where, on)\n",
    "- po order by\n",
    "\n",
    "Nazwy tabel:\n",
    "- tam gdzie nazwy kolumn\n",
    "- po from\n",
    "- po join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"select horsepower ,  T1.Make FROM CAR_NAMES AS T1 JOIN CARS_DATA AS T2 ON T1.MakeId  =  T2.Id WHERE T2.cylinders  =  3 ORDER BY T2.horsepower DESC LIMIT 1;\"\n",
    "q2 = \"SELECT LOCATION ,  name FROM stadium WHERE capacity BETWEEN 5000 AND 10000\"\n",
    "q3 = \"select max(capacity) from stadium as ST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('screen_mode', 'T1'), ('phone', 'T2')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tables_aliasing(sql):\n",
    "    aliasing = []\n",
    "    tokens = [token for token in sqlparse.parse(sql)[0].flatten() if str(token).strip() != '']\n",
    "    for i in range(1, len(tokens)-1):\n",
    "        prev, current, next = tokens[i-1:i+2]\n",
    "        if str(current).upper() == 'AS':\n",
    "            assert str(current.ttype) == 'Token.Keyword'\n",
    "            aliasing.append((str(prev), str(next))) \n",
    "    return aliasing\n",
    "\n",
    "# get_tables_aliasing(q1)\n",
    "\n",
    "q5 = 'SELECT T1.name ,  T1.id FROM station AS T1 JOIN status AS T2 ON T1.id  =  T2.station_id GROUP BY T2.station_id HAVING avg(T2.bikes_available)  >  14 UNION SELECT name ,  id FROM station WHERE installation_date LIKE \"12/%\"'\n",
    "q6 = 'SELECT count(*) FROM station AS T1 JOIN trip AS T2 JOIN station AS T3 JOIN trip AS T4 ON T1.id  =  T2.start_station_id AND T2.id  =  T4.id AND T3.id  =  T4.end_station_id WHERE T1.city  =  \"Mountain View\" AND T3.city  =  \"Palo Alto\"'\n",
    "q7 = 'SELECT DISTINCT T2.Hardware_Model_name FROM screen_mode AS T1 JOIN phone AS T2 ON T1.Graphics_mode = T2.screen_mode WHERE T1.Type  =  \"Graphics\" OR t2.Company_name  =  \"Nokia Corporation\"'\n",
    "get_tables_aliasing(q7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CAR_NAMES': 'T1', 'CARS_DATA': 'T2'}"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def get_tables_aliasing(sql):\n",
    "#     parsed = sqlglot.parse_one(sql)\n",
    "#     aliasing = {table.this.output_name: table.alias for table in parsed.find_all(exp.Table)}\n",
    "#     return aliasing\n",
    "\n",
    "# get_tables_aliasing(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('station', 'T1'), ('trip', 'T2'), ('station', 'T3'), ('trip', 'T4')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q5 = 'SELECT T1.name ,  T1.id FROM station AS T1 JOIN status AS T2 ON T1.id  =  T2.station_id GROUP BY T2.station_id HAVING avg(T2.bikes_available)  >  14 UNION SELECT name ,  id FROM station WHERE installation_date LIKE \"12/%\"'\n",
    "q6 = 'SELECT count(*) FROM station AS T1 JOIN trip AS T2 JOIN station AS T3 JOIN trip AS T4 ON T1.id  =  T2.start_station_id AND T2.id  =  T4.id AND T3.id  =  T4.end_station_id WHERE T1.city  =  \"Mountain View\" AND T3.city  =  \"Palo Alto\"'\n",
    "get_tables_aliasing(q6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dorm', 'dorm_amenity', 'has_amenity', 'lives_in', 'student'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_tables(sql):\n",
    "    parsed = sqlglot.parse_one(sql)\n",
    "    tables = {table.this.output_name for table in parsed.find_all(exp.Table)}\n",
    "    return tables\n",
    "\n",
    "find_tables(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4 = \"SELECT student_id FROM students WHERE student_id NOT IN (SELECT student_id FROM student_course_attendance)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amenid', 'dormid', 'fname', 'stuid'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_columns(sql):\n",
    "    parsed = sqlglot.parse_one(sql)\n",
    "    columns = {col.output_name for col in parsed.find_all(exp.Column)}\n",
    "    return columns\n",
    "\n",
    "find_columns(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_nested_query_tokens(tokens):\n",
    "    outer_query = []\n",
    "    active_nested_query = []\n",
    "    nested_queries = []\n",
    "    \n",
    "    is_in_nested = False\n",
    "    parenth_level = 0\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        \n",
    "        if str(token) == '(':\n",
    "            parenth_level += 1\n",
    "            \n",
    "        elif str(token) == ')':\n",
    "            parenth_level -= 1\n",
    "            \n",
    "            if parenth_level == 0 and is_in_nested:\n",
    "                nested_queries.append(active_nested_query)\n",
    "                active_nested_query = []\n",
    "                is_in_nested = False\n",
    "            \n",
    "        elif str(token).upper() == 'SELECT':\n",
    "            if parenth_level == 1 and i > 0 and str(tokens[i-1]) == '(':\n",
    "                is_in_nested = True\n",
    "                \n",
    "        if is_in_nested:\n",
    "            active_nested_query.append(token)\n",
    "        else:\n",
    "            outer_query.append(token)\n",
    "            \n",
    "        i += 1\n",
    "        \n",
    "    return outer_query, nested_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<DML 'SELECT' at 0x26F9D800BE0>,\n",
       "  <Wildcard '*' at 0x26FC0187580>,\n",
       "  <Keyword 'FROM' at 0x26FC0187880>,\n",
       "  <Name 'studen...' at 0x26FC0187D60>,\n",
       "  <Keyword 'WHERE' at 0x26FC0187E20>,\n",
       "  <Name 'studen...' at 0x26FC0187EE0>,\n",
       "  <Keyword 'NOT' at 0x26FC0187FA0>,\n",
       "  <Keyword 'IN' at 0x26FC0185A20>,\n",
       "  <Punctuation '(' at 0x26FC0185AE0>,\n",
       "  <Punctuation ')' at 0x26FC01854E0>],\n",
       " [[<DML 'SELECT' at 0x26FC01856C0>,\n",
       "   <Name 'studen...' at 0x26FC0185780>,\n",
       "   <Keyword 'FROM' at 0x26FC0185840>,\n",
       "   <Name 'studen...' at 0x26FC0185420>]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement = sqlparse.parse('SELECT * FROM student_course_registrations WHERE student_id NOT IN ( SELECT student_id FROM student_course_attendance )')[0]\n",
    "tokens = [token for token in statement.flatten() if str(token).strip() != '']\n",
    "split_nested_query_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_set_query_tokens(tokens):\n",
    "    subqueries = []\n",
    "    current_query = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if str(token).upper() in ['UNION', 'EXCEPT', 'INTERSECT']:\n",
    "            subqueries.append(current_query)\n",
    "            current_query = []\n",
    "        else:\n",
    "            current_query.append(token)\n",
    "    subqueries.append(current_query)\n",
    "    return subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_set_query_tokens(tokens):\n",
    "    active_query = []\n",
    "    queries = []\n",
    "    \n",
    "    parenth_level = 0\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        \n",
    "        if str(token) == '(':\n",
    "            parenth_level += 1\n",
    "            \n",
    "        elif str(token) == ')':\n",
    "            parenth_level -= 1\n",
    "            \n",
    "        if str(token).upper() in ['UNION', 'EXCEPT', 'INTERSECT'] and parenth_level == 0:\n",
    "            queries.append(active_query)\n",
    "            active_query = []\n",
    "        else:        \n",
    "            active_query.append(token)\n",
    "            \n",
    "        i += 1\n",
    "        \n",
    "    queries.append(active_query)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<DML 'SELECT' at 0x26FC08731C0>,\n",
       "  <Name 'studen...' at 0x26FC3232860>,\n",
       "  <Keyword 'FROM' at 0x26FC3231960>,\n",
       "  <Name 'studen...' at 0x26FC3231B40>],\n",
       " [<DML 'SELECT' at 0x26FC3231DE0>,\n",
       "  <Name 'studen...' at 0x26FC3231EA0>,\n",
       "  <Keyword 'FROM' at 0x26FC3231F60>,\n",
       "  <Name 'studen...' at 0x26FC3232020>]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement = sqlparse.parse('SELECT student_id FROM student_course_registrations UNION SELECT student_id FROM student_course_attendance')[0]\n",
    "tokens = [token for token in statement.flatten() if str(token).strip() != '']\n",
    "split_set_query_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_simple_query_tokens(tokens, db_id, tables_trans_dict, columns_trans_dict, parent_aliasing=None):\n",
    "    parent_aliasing = parent_aliasing or []\n",
    "    \n",
    "    tables_trans_dict = tables_trans_dict[db_id]\n",
    "    columns_trans_dict = columns_trans_dict[db_id]\n",
    "        \n",
    "    query = ' '.join([str(token) for token in tokens])\n",
    "    aliasing = get_tables_aliasing(query)\n",
    "    aliasing_rev = {new.lower(): old for old, new in aliasing}\n",
    "    parent_aliasing_rev = {new.lower(): old for old, new in parent_aliasing}\n",
    "    columns_names = find_columns(query.replace('( )', '(select 1)'))\n",
    "    tables_names = find_tables(query.replace('( )', '(select 1)'))\n",
    "    if len(columns_names | tables_names) != len(columns_names) + len(tables_names):\n",
    "        print('.', end='')\n",
    "    \n",
    "    for i in reversed(range(len(tokens))):\n",
    "        if str(tokens[i].ttype).startswith('Token.Literal.String'): # skip if token is string value\n",
    "            continue\n",
    "        \n",
    "        if str(tokens[i]) in columns_names and str(tokens[i-1]).lower() not in ['FROM', 'JOIN']:\n",
    "            table_name = str(tokens[i-2]) if i >= 2 and str(tokens[i-1]) == '.' else None\n",
    "            if table_name is not None:\n",
    "                if table_name.lower() in aliasing_rev:\n",
    "                    table_name = aliasing_rev[table_name.lower()]\n",
    "                elif table_name.lower() in parent_aliasing_rev:\n",
    "                    table_name = parent_aliasing_rev[table_name.lower()]\n",
    "            \n",
    "            if table_name is None:\n",
    "                possible_table_names = [table_name for table_name, table_trans in columns_trans_dict.items() if table_name in [x.lower() for x in tables_names] and str(tokens[i]).lower() in table_trans]\n",
    "                assert len(possible_table_names) > 0\n",
    "                if len(possible_table_names) > 1:\n",
    "                    # print(str(tokens[i-1]))\n",
    "                    # print(str(tokens[i-2]))\n",
    "                    # print(possible_table_names)\n",
    "                    # print('+', query, '|', str(tokens[i]), f'({db_id})')\n",
    "                    pass\n",
    "                table_name = possible_table_names[0]\n",
    "            column_name_pl = columns_trans_dict[table_name.lower()][str(tokens[i]).lower()]['name_original']\n",
    "            tokens[i].value = column_name_pl\n",
    "            \n",
    "        elif str(tokens[i]) in tables_names:\n",
    "            table_name_pl = tables_trans_dict[str(tokens[i]).lower()]['name_original']\n",
    "            tokens[i].value = table_name_pl\n",
    "            \n",
    "    return aliasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {1, 2}\n",
    "b = {2, 3}\n",
    "a|b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_query_tokens(tokens, db_id, tables_trans_dict, columns_trans_dict, aliasing):\n",
    "    set_queries = split_set_query_tokens(tokens)\n",
    "    for set_query in set_queries:\n",
    "        outer_query, nested_queries = split_nested_query_tokens(set_query)\n",
    "        \n",
    "        outer_aliasing = translate_simple_query_tokens(outer_query, db_id, tables_trans_dict, columns_trans_dict, aliasing)\n",
    "    \n",
    "        for query in nested_queries:\n",
    "            translate_query_tokens(query, db_id, tables_trans_dict, columns_trans_dict, [*aliasing, *outer_aliasing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_query(query, db_id, tables_trans_dict, columns_trans_dict):\n",
    "    statement = sqlparse.parse(query)[0]\n",
    "    tokens = [token for token in statement.flatten() if str(token).strip() != '']\n",
    "    translate_query_tokens(tokens, db_id, tables_trans_dict, columns_trans_dict, [])\n",
    "    return str(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SELECT DISTINCT T2.Imię FROM Przechodzi AS T1 JOIN Pacjent AS T2 ON T1.Pacjent = T2.SSN JOIN Pobyt AS T3 ON T1.Pobyt  =  T3.PobytID WHERE T3.Pokój  =  111'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q0 = \"SELECT student_id FROM student_course_registrations UNION SELECT student_id FROM student_course_attendance intersect SELECT student_id FROM student_course_attendance\"\n",
    "q1 = \"SELECT T1.fname FROM student AS T1 JOIN lives_in AS T2 ON T1.stuid  =  T2.stuid WHERE T2.dormid IN (SELECT T2.dormid FROM dorm AS T3 JOIN has_amenity AS T4 ON T3.dormid  =  T4.dormid JOIN dorm_amenity AS T5 ON T4.amenid  =  T5.amenid GROUP BY T3.dormid ORDER BY count(*) DESC LIMIT 1)\"\n",
    "q2 = \"SELECT DISTINCT T2.name FROM undergoes AS T1 JOIN patient AS T2 ON T1.patient = T2.SSN JOIN stay AS T3 ON T1.Stay  =  T3.StayID WHERE T3.room  =  111\"\n",
    "translate_query(\n",
    "    q2,\n",
    "    # \"student_assessment\",\n",
    "    # \"dorm_1\",\n",
    "    \"hospital_1\",\n",
    "    create_tables_translation_dict('../auxiliary/translated_schema/tables_names.json'),\n",
    "    create_columns_translation_dict('../auxiliary/translated_schema/columns_names.json'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_samples(samples, tables_translation_dict, columns_translation_dict):\n",
    "    for sample in samples:\n",
    "        # print(sample['query'])\n",
    "        query_pl = translate_query(sample['query'], sample['db_id'], tables_translation_dict, columns_translation_dict)\n",
    "        sample['query'] = query_pl\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "with open('../auxiliary/translated_likes/train_spider.json') as f:\n",
    "    samples = json.load(f)\n",
    "    \n",
    "tables_translation_dict = create_tables_translation_dict('../auxiliary/translated_schema/tables_names.json')\n",
    "columns_translation_dict = create_columns_translation_dict('../auxiliary/translated_schema/columns_names.json')\n",
    "\n",
    "trans_samples = translate_samples(samples, tables_translation_dict, columns_translation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../auxiliary/translated_likes/train_spider.json', 'w') as f:\n",
    "        json.dump(trans_samples, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_complete(src_name, dst_name):\n",
    "    src_path = Path('../auxiliary') / src_name\n",
    "    dst_path = Path('../complete') / dst_name\n",
    "    \n",
    "    dst_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    for file in ['dev.json', 'train_others.json', 'train_spider.json']:\n",
    "        add_calculated_attributes_to_samples(\n",
    "            src_path / file,\n",
    "            dst_path / file,\n",
    "            src_path / 'tables.json'\n",
    "        )\n",
    "\n",
    "    create_gold_sql(\n",
    "        [\n",
    "            src_path / 'dev.json'\n",
    "        ],\n",
    "        dst_path / 'dev_gold.sql'\n",
    "    )\n",
    "\n",
    "    create_gold_sql(\n",
    "        [\n",
    "            src_path / 'train_spider.json',\n",
    "            src_path / 'train_others.json'\n",
    "        ],\n",
    "        dst_path / 'train_gold.sql'\n",
    "    )\n",
    "    \n",
    "    shutil.copyfile(src_path / 'tables.json', dst_path / 'tables.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_complete('machine_translated', 'machine_translated')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polish-spider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
