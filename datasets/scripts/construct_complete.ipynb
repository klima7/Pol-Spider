{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "\n",
    "import sqlparse\n",
    "import spacy\n",
    "from sqlparse import sql\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from process_sql import create_sql, SQLParseException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading whole english spider dataset for further tests\n",
    "spider_en = []\n",
    "with open('../../spider-en/train_spider.json') as json_data:\n",
    "    spider_en.extend(json.load(json_data))\n",
    "with open('../../spider-en/train_others.json') as json_data:\n",
    "    spider_en.extend(json.load(json_data))\n",
    "with open('../../spider-en/dev.json') as json_data:\n",
    "    spider_en.extend(json.load(json_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query toks no value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_query_no_value(query):\n",
    "    statement = sqlparse.parse(query)[0]\n",
    "    \n",
    "    value_tokens = [token for token in statement.flatten() if str(token.ttype).startswith('Token.Literal')]\n",
    "    for token in value_tokens:\n",
    "        token.value = 'value'\n",
    "        \n",
    "    coarse_tokens =  [str(token).lower() for token in statement.flatten() if str(token).strip() != '']\n",
    "    \n",
    "    fine_tokens = []\n",
    "    for token in coarse_tokens:\n",
    "        if len(token.split(' ')) > 1:\n",
    "            fine_tokens.extend(token.split(' '))\n",
    "        elif token == '!=':\n",
    "            fine_tokens.extend(['!', '='])\n",
    "        elif token == '>=':\n",
    "            fine_tokens.extend(['>', '='])\n",
    "        elif token == '<=':\n",
    "            fine_tokens.extend(['<', '='])\n",
    "        elif token == ';':\n",
    "            continue\n",
    "        else:\n",
    "            fine_tokens.append(token)\n",
    "    return fine_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare created tokenization function with tokens from oryginal spider\n",
    "discrepancies = 0\n",
    "\n",
    "for sample in spider_en:\n",
    "    my_tokens = tokenize_query_no_value(sample['query'])\n",
    "    oryginal_tokens = sample['query_toks_no_value']\n",
    "    if my_tokens != oryginal_tokens:\n",
    "        discrepancies += 1\n",
    "        print(sample['query'])\n",
    "        print(my_tokens)\n",
    "        print(oryginal_tokens)\n",
    "        print()\n",
    "        \n",
    "print(discrepancies, 'discrepancies')\n",
    "# found 18 discrepancies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pl = spacy.load(\"pl_core_news_md\")\n",
    "\n",
    "def tokenize_polish(text):\n",
    "    return [str(token) for token in nlp_pl(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_statement(root, tokens=None):\n",
    "    if tokens is None:\n",
    "        tokens = []\n",
    "    \n",
    "    # Create tokens from elements which we can't divide\n",
    "    if not hasattr(root, 'tokens'):\n",
    "        token = str(root).strip()\n",
    "        if token == '' or token == ';':\n",
    "            pass\n",
    "        elif len(token.split(' ')) > 1:\n",
    "            tokens.extend(token.split(' '))\n",
    "        elif token == '!=':\n",
    "            tokens.extend(['!', '='])\n",
    "        elif token == '>=':\n",
    "            tokens.extend(['>', '='])\n",
    "        elif token == '<=':\n",
    "            tokens.extend(['<', '='])\n",
    "        else:\n",
    "            tokens.append(str(root))\n",
    "    \n",
    "    # Not split identifiers like \"T1.name\" into separate tokens\n",
    "    elif isinstance(root, sql.Identifier) and '.' in str(root):\n",
    "        tokens.append(str(root))\n",
    "        \n",
    "    # Tokenize strings using polish tokenizer\n",
    "    elif isinstance(root, sql.Identifier) and len(root.tokens) == 1 and str(root.tokens[0].ttype) == 'Token.Literal.String.Symbol':\n",
    "        tokens.extend(tokenize_polish(str(root)))\n",
    "        \n",
    "    # Tokenize other compound elements recursively\n",
    "    else:\n",
    "        for token in root.tokens:\n",
    "            tokenize_statement(token, tokens)\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_query(query):\n",
    "    statement = sqlparse.parse(query)[0]\n",
    "    tokens = tokenize_statement(statement)\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare created tokenization function with tokens from oryginal spider\n",
    "discrepancies = 0\n",
    "\n",
    "for sample in spider_en:\n",
    "    my_tokens = tokenize_query(sample['query'])\n",
    "    oryginal_tokens = sample['query_toks']\n",
    "    if my_tokens != oryginal_tokens:\n",
    "        discrepancies += 1\n",
    "        print(sample['query'])\n",
    "        print(my_tokens)\n",
    "        print(oryginal_tokens)\n",
    "        print()\n",
    "        \n",
    "print(discrepancies, 'discrepancies')\n",
    "# found 4754 discrepancies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_question(question):\n",
    "    return tokenize_polish(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_calculated_attributes_to_samples(src_path, tgt_path, tables_path):\n",
    "    with open(src_path) as f:\n",
    "        samples = json.load(f)\n",
    "        \n",
    "    new_samples = []\n",
    "        \n",
    "    for sample in tqdm(samples):\n",
    "        try:\n",
    "            sql = create_sql(sample['db_id'], sample['query_pl'], tables_path)\n",
    "        except SQLParseException:\n",
    "            # skip samples with invalid sqls\n",
    "            print('Skipping sample')\n",
    "        \n",
    "        new_sample = {\n",
    "            'db_id': sample['db_id'],\n",
    "            'question': sample['question_pl'],\n",
    "            'question_toks': tokenize_question(sample['question_pl']),\n",
    "            'query': sample['query_pl'],\n",
    "            'query_toks': tokenize_query(sample['query_pl']),\n",
    "            'query_toks_no_value': tokenize_query_no_value(sample['query_pl']),\n",
    "            'sql': sql\n",
    "        }\n",
    "        \n",
    "        new_samples.append(new_sample)\n",
    "        \n",
    "    with open(tgt_path, 'w') as f:\n",
    "        json.dump(new_samples, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_sql(src_files, tgt_file):\n",
    "    with open(tgt_file, 'w') as f:\n",
    "        for src_file in src_files:\n",
    "            with open(src_file) as g:\n",
    "                for sample in json.load(g):\n",
    "                    f.write(f\"{sample['query_pl']}\\t{sample['db_id']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tables(tables_path, tables_names_path, columns_names_path, output_path):\n",
    "    with open(tables_names_path) as f:\n",
    "        tables_names = json.load(f)\n",
    "        \n",
    "    tables_names_dict = {}\n",
    "    for entry in tables_names:\n",
    "        if entry['db_id'] not in tables_names_dict:\n",
    "            tables_names_dict[entry['db_id']] = {}\n",
    "        tables_names_dict[entry['db_id']][entry['name_original']] = {'name': entry['name_original_pl'], 'name_original': entry['name_pl']}\n",
    "\n",
    "    with open(columns_names_path) as f:\n",
    "        columns_names = json.load(f)\n",
    "        \n",
    "    columns_names_dict = {}\n",
    "    for entry in columns_names:\n",
    "        if entry['db_id'] not in columns_names_dict:\n",
    "            columns_names_dict[entry['db_id']] = {}\n",
    "        db_stuff = columns_names_dict[entry['db_id']]\n",
    "        if entry['table_name'] not in db_stuff:\n",
    "            db_stuff[entry['table_name']] = {}\n",
    "        table_stuff = db_stuff[entry['table_name']]\n",
    "        table_stuff[entry['column_name_original']] = {'name': entry['column_name_original_pl'], 'name_original': entry['column_name_pl']}\n",
    "        \n",
    "    with open(tables_path) as f:\n",
    "        tables_json = json.load(f)\n",
    "        \n",
    "    # perform translation\n",
    "    for db in tables_json:\n",
    "        db_id = db['db_id']\n",
    "        \n",
    "        # translate columns\n",
    "        assert db['column_names'][0][1] == '*'\n",
    "        assert db['column_names_original'][0][1] == '*'\n",
    "        for i in range(1, len(db['column_names_original'])):\n",
    "            table_idx, column_name_original = db['column_names_original'][i]\n",
    "            table_name = db['table_names'][table_idx]\n",
    "            translations = columns_names_dict[db_id][table_name][column_name_original]\n",
    "            db['column_names_original'][i][1] = translations['name_original']\n",
    "            db['column_names'][i][1] = translations['name']\n",
    "        \n",
    "        # translate tables\n",
    "        for i in range(len(db['table_names_original'])):\n",
    "            translations = tables_names_dict[db_id][db['table_names_original'][i]]\n",
    "            db['table_names_original'][i] = translations['name']\n",
    "            db['table_names'][i] = translations['name_original']\n",
    "        \n",
    "    # save translated\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(tables_json, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = translate_tables(\n",
    "    '../auxiliary/translated_schema/tables.json',\n",
    "    '../auxiliary/translated_schema/tables_names.json',\n",
    "    '../auxiliary/translated_schema/columns_names.json',\n",
    "    '../auxiliary/translated_likes/tables.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_complete(src_name, dst_name):\n",
    "    src_path = Path('../auxiliary') / src_name\n",
    "    dst_path = Path('../complete') / dst_name\n",
    "    \n",
    "    dst_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    for file in ['dev.json', 'train_others.json', 'train_spider.json']:\n",
    "        add_calculated_attributes_to_samples(\n",
    "            src_path / file,\n",
    "            dst_path / file,\n",
    "            src_path / 'tables.json'\n",
    "        )\n",
    "\n",
    "    create_gold_sql(\n",
    "        [\n",
    "            src_path / 'dev.json'\n",
    "        ],\n",
    "        dst_path / 'dev_gold.sql'\n",
    "    )\n",
    "\n",
    "    create_gold_sql(\n",
    "        [\n",
    "            src_path / 'train_spider.json',\n",
    "            src_path / 'train_others.json'\n",
    "        ],\n",
    "        dst_path / 'train_gold.sql'\n",
    "    )\n",
    "    \n",
    "    shutil.copyfile(src_path / 'tables.json', dst_path / 'tables.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_complete('machine_translated', 'machine_translated')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polish-spider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
