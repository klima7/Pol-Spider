{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import sqlparse\n",
    "from sqlparse import sql\n",
    "import sqlglot\n",
    "import sqlglot.expressions as exp\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from process_sql import create_sql, SQLParseException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading whole english spider dataset for further tests\n",
    "spider_en = []\n",
    "with open('../../spider-en/train_spider.json') as json_data:\n",
    "    spider_en.extend(json.load(json_data))\n",
    "with open('../../spider-en/train_others.json') as json_data:\n",
    "    spider_en.extend(json.load(json_data))\n",
    "with open('../../spider-en/dev.json') as json_data:\n",
    "    spider_en.extend(json.load(json_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query toks no value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_query_no_value(query):\n",
    "    statement = sqlparse.parse(query)[0]\n",
    "    \n",
    "    value_tokens = [token for token in statement.flatten() if str(token.ttype).startswith('Token.Literal')]\n",
    "    for token in value_tokens:\n",
    "        token.value = 'value'\n",
    "        \n",
    "    coarse_tokens =  [str(token).lower() for token in statement.flatten() if str(token).strip() != '']\n",
    "    \n",
    "    fine_tokens = []\n",
    "    for token in coarse_tokens:\n",
    "        if len(token.split(' ')) > 1:\n",
    "            fine_tokens.extend(token.split(' '))\n",
    "        elif token == '!=':\n",
    "            fine_tokens.extend(['!', '='])\n",
    "        elif token == '>=':\n",
    "            fine_tokens.extend(['>', '='])\n",
    "        elif token == '<=':\n",
    "            fine_tokens.extend(['<', '='])\n",
    "        elif token == ';':\n",
    "            continue\n",
    "        else:\n",
    "            fine_tokens.append(token)\n",
    "    return fine_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare created tokenization function with tokens from oryginal spider\n",
    "discrepancies = 0\n",
    "\n",
    "for sample in spider_en:\n",
    "    my_tokens = tokenize_query_no_value(sample['query'])\n",
    "    oryginal_tokens = sample['query_toks_no_value']\n",
    "    if my_tokens != oryginal_tokens:\n",
    "        discrepancies += 1\n",
    "        print(sample['query'])\n",
    "        print(my_tokens)\n",
    "        print(oryginal_tokens)\n",
    "        print()\n",
    "        \n",
    "print(discrepancies, 'discrepancies')\n",
    "# found 18 discrepancies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pl = spacy.load(\"pl_core_news_md\")\n",
    "\n",
    "def tokenize_polish(text):\n",
    "    return [str(token) for token in nlp_pl(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_statement(root, tokens=None):\n",
    "    if tokens is None:\n",
    "        tokens = []\n",
    "    \n",
    "    # Create tokens from elements which we can't divide\n",
    "    if not hasattr(root, 'tokens'):\n",
    "        token = str(root).strip()\n",
    "        if token == '' or token == ';':\n",
    "            pass\n",
    "        elif len(token.split(' ')) > 1:\n",
    "            tokens.extend(token.split(' '))\n",
    "        elif token == '!=':\n",
    "            tokens.extend(['!', '='])\n",
    "        elif token == '>=':\n",
    "            tokens.extend(['>', '='])\n",
    "        elif token == '<=':\n",
    "            tokens.extend(['<', '='])\n",
    "        else:\n",
    "            tokens.append(str(root))\n",
    "    \n",
    "    # Not split identifiers like \"T1.name\" into separate tokens\n",
    "    elif isinstance(root, sql.Identifier) and '.' in str(root):\n",
    "        tokens.append(str(root))\n",
    "        \n",
    "    # Tokenize strings using polish tokenizer\n",
    "    elif isinstance(root, sql.Identifier) and len(root.tokens) == 1 and str(root.tokens[0].ttype) == 'Token.Literal.String.Symbol':\n",
    "        tokens.extend(tokenize_polish(str(root)))\n",
    "        \n",
    "    # Tokenize other compound elements recursively\n",
    "    else:\n",
    "        for token in root.tokens:\n",
    "            tokenize_statement(token, tokens)\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_query(query):\n",
    "    statement = sqlparse.parse(query)[0]\n",
    "    tokens = tokenize_statement(statement)\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare created tokenization function with tokens from oryginal spider\n",
    "discrepancies = 0\n",
    "\n",
    "for sample in spider_en:\n",
    "    my_tokens = tokenize_query(sample['query'])\n",
    "    oryginal_tokens = sample['query_toks']\n",
    "    if my_tokens != oryginal_tokens:\n",
    "        discrepancies += 1\n",
    "        print(sample['query'])\n",
    "        print(my_tokens)\n",
    "        print(oryginal_tokens)\n",
    "        print()\n",
    "        \n",
    "print(discrepancies, 'discrepancies')\n",
    "# found 4754 discrepancies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_question(question):\n",
    "    return tokenize_polish(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_calculated_attributes_to_samples(src_path, tgt_path, tables_path):\n",
    "    with open(src_path) as f:\n",
    "        samples = json.load(f)\n",
    "        \n",
    "    new_samples = []\n",
    "        \n",
    "    for sample in tqdm(samples):\n",
    "        try:\n",
    "            sql = create_sql(sample['db_id'], sample['query_pl'], tables_path)\n",
    "        except SQLParseException:\n",
    "            # skip samples with invalid sqls\n",
    "            print('Skipping sample')\n",
    "        \n",
    "        new_sample = {\n",
    "            'db_id': sample['db_id'],\n",
    "            'question': sample['question_pl'],\n",
    "            'question_toks': tokenize_question(sample['question_pl']),\n",
    "            'query': sample['query_pl'],\n",
    "            'query_toks': tokenize_query(sample['query_pl']),\n",
    "            'query_toks_no_value': tokenize_query_no_value(sample['query_pl']),\n",
    "            'sql': sql\n",
    "        }\n",
    "        \n",
    "        new_samples.append(new_sample)\n",
    "        \n",
    "    with open(tgt_path, 'w') as f:\n",
    "        json.dump(new_samples, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_sql(src_files, tgt_file):\n",
    "    with open(tgt_file, 'w') as f:\n",
    "        for src_file in src_files:\n",
    "            with open(src_file) as g:\n",
    "                for sample in json.load(g):\n",
    "                    f.write(f\"{sample['query_pl']}\\t{sample['db_id']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables_translation_dict(tables_names_path):\n",
    "    with open(tables_names_path) as f:\n",
    "        tables_names = json.load(f)\n",
    "        \n",
    "    tables_trans_dict = {}\n",
    "    for entry in tables_names:\n",
    "        if entry['db_id'] not in tables_trans_dict:\n",
    "            tables_trans_dict[entry['db_id']] = {}\n",
    "        tables_trans_dict[entry['db_id']][entry['name_original'].lower()] = {'name': entry['name_pl'], 'name_original': entry['name_original_pl']}\n",
    "        \n",
    "    return tables_trans_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_columns_translation_dict(columns_names_path):\n",
    "    with open(columns_names_path) as f:\n",
    "        columns_names = json.load(f)\n",
    "        \n",
    "    columns_trains_dict = {}\n",
    "    for entry in columns_names:\n",
    "        if entry['db_id'] not in columns_trains_dict:\n",
    "            columns_trains_dict[entry['db_id']] = {}\n",
    "        db_stuff = columns_trains_dict[entry['db_id']]\n",
    "        if entry['table_name_original'].lower() not in db_stuff:\n",
    "            db_stuff[entry['table_name_original'].lower()] = {}\n",
    "        table_stuff = db_stuff[entry['table_name_original'].lower()]\n",
    "        table_stuff[entry['column_name_original'].lower()] = {'name': entry['column_name_pl'], 'name_original': entry['column_name_original_pl']}\n",
    "        \n",
    "    return columns_trains_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables_translation_dict = create_tables_translation_dict('../auxiliary/translated_schema/tables_names.json')\n",
    "columns_translation_dict = create_columns_translation_dict('../auxiliary/translated_schema/columns_names.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perpetrator_id': {'name': 'id sprawcy', 'name_original': 'Sprawca_ID'},\n",
       " 'people_id': {'name': 'id osoby', 'name_original': 'Ludzie_ID'},\n",
       " 'date': {'name': 'data', 'name_original': 'Data'},\n",
       " 'year': {'name': 'rok', 'name_original': 'Rok'},\n",
       " 'location': {'name': 'lokalizacja', 'name_original': 'Lokalizacja'},\n",
       " 'country': {'name': 'kraj', 'name_original': 'Kraj'},\n",
       " 'killed': {'name': 'zabity', 'name_original': 'Zabity'},\n",
       " 'injured': {'name': 'obrażenia', 'name_original': 'Obrażenia'}}"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_translation_dict['perpetrator']['perpetrator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tables(tables_path, tables_names_path, columns_names_path, output_path):\n",
    "    tables_trans_dict = create_tables_translation_dict(tables_names_path)\n",
    "    columns_trans_dict = create_columns_translation_dict(columns_names_path)\n",
    "        \n",
    "    with open(tables_path) as f:\n",
    "        tables_json = json.load(f)\n",
    "        \n",
    "    # perform translation\n",
    "    for db in tables_json:\n",
    "        db_id = db['db_id']\n",
    "        \n",
    "        # translate columns\n",
    "        assert db['column_names'][0][1] == '*'\n",
    "        assert db['column_names_original'][0][1] == '*'\n",
    "        for i in range(1, len(db['column_names_original'])):\n",
    "            table_idx, column_name_original = db['column_names_original'][i]\n",
    "            table_name = db['table_names_original'][table_idx]\n",
    "            translations = columns_trans_dict[db_id][table_name][column_name_original]\n",
    "            db['column_names_original'][i][1] = translations['name_original']\n",
    "            db['column_names'][i][1] = translations['name']\n",
    "        \n",
    "        # translate tables\n",
    "        for i in range(len(db['table_names_original'])):\n",
    "            table_name = db['table_names_original'][i]\n",
    "            translations = tables_trans_dict[db_id][table_name]\n",
    "            db['table_names_original'][i] = translations['name']\n",
    "            db['table_names'][i] = translations['name_original']\n",
    "        \n",
    "    # save translated\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(tables_json, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = translate_tables(\n",
    "    '../auxiliary/translated_schema/tables.json',\n",
    "    '../auxiliary/translated_schema/tables_names.json',\n",
    "    '../auxiliary/translated_schema/columns_names.json',\n",
    "    '../auxiliary/translated_likes/tables.json'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nazwy kolumn:\n",
    "- po select\n",
    "    - w funkcjach agregacyjnych (min, max, avg)\n",
    "- w porównaniach (where, on)\n",
    "- po order by\n",
    "\n",
    "Nazwy tabel:\n",
    "- tam gdzie nazwy kolumn\n",
    "- po from\n",
    "- po join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"select horsepower ,  T1.Make FROM CAR_NAMES AS T1 JOIN CARS_DATA AS T2 ON T1.MakeId  =  T2.Id WHERE T2.cylinders  =  3 ORDER BY T2.horsepower DESC LIMIT 1;\"\n",
    "q2 = \"SELECT LOCATION ,  name FROM stadium WHERE capacity BETWEEN 5000 AND 10000\"\n",
    "q3 = \"select max(capacity) from stadium as ST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('screen_mode', 'T1'), ('phone', 'T2')]"
      ]
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tables_aliasing(sql):\n",
    "    aliasing = []\n",
    "    tokens = [token for token in sqlparse.parse(sql)[0].flatten() if str(token).strip() != '']\n",
    "    for i in range(1, len(tokens)-1):\n",
    "        prev, current, next = tokens[i-1:i+2]\n",
    "        if str(current).upper() == 'AS':\n",
    "            assert str(current.ttype) == 'Token.Keyword'\n",
    "            aliasing.append((str(prev), str(next))) \n",
    "    return aliasing\n",
    "\n",
    "# get_tables_aliasing(q1)\n",
    "\n",
    "q5 = 'SELECT T1.name ,  T1.id FROM station AS T1 JOIN status AS T2 ON T1.id  =  T2.station_id GROUP BY T2.station_id HAVING avg(T2.bikes_available)  >  14 UNION SELECT name ,  id FROM station WHERE installation_date LIKE \"12/%\"'\n",
    "q6 = 'SELECT count(*) FROM station AS T1 JOIN trip AS T2 JOIN station AS T3 JOIN trip AS T4 ON T1.id  =  T2.start_station_id AND T2.id  =  T4.id AND T3.id  =  T4.end_station_id WHERE T1.city  =  \"Mountain View\" AND T3.city  =  \"Palo Alto\"'\n",
    "q7 = 'SELECT DISTINCT T2.Hardware_Model_name FROM screen_mode AS T1 JOIN phone AS T2 ON T1.Graphics_mode = T2.screen_mode WHERE T1.Type  =  \"Graphics\" OR t2.Company_name  =  \"Nokia Corporation\"'\n",
    "get_tables_aliasing(q7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CAR_NAMES': 'T1', 'CARS_DATA': 'T2'}"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def get_tables_aliasing(sql):\n",
    "#     parsed = sqlglot.parse_one(sql)\n",
    "#     aliasing = {table.this.output_name: table.alias for table in parsed.find_all(exp.Table)}\n",
    "#     return aliasing\n",
    "\n",
    "# get_tables_aliasing(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('station', 'T1'), ('trip', 'T2'), ('station', 'T3'), ('trip', 'T4')]"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q5 = 'SELECT T1.name ,  T1.id FROM station AS T1 JOIN status AS T2 ON T1.id  =  T2.station_id GROUP BY T2.station_id HAVING avg(T2.bikes_available)  >  14 UNION SELECT name ,  id FROM station WHERE installation_date LIKE \"12/%\"'\n",
    "q6 = 'SELECT count(*) FROM station AS T1 JOIN trip AS T2 JOIN station AS T3 JOIN trip AS T4 ON T1.id  =  T2.start_station_id AND T2.id  =  T4.id AND T3.id  =  T4.end_station_id WHERE T1.city  =  \"Mountain View\" AND T3.city  =  \"Palo Alto\"'\n",
    "get_tables_aliasing(q6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CAR_NAMES', 'CARS_DATA']"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_tables(sql):\n",
    "    parsed = sqlglot.parse_one(sql)\n",
    "    tables = [table.this.output_name for table in parsed.find_all(exp.Table)]\n",
    "    return tables\n",
    "\n",
    "find_tables(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4 = \"SELECT student_id FROM students WHERE student_id NOT IN (SELECT student_id FROM student_course_attendance)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['horsepower', 'Make', 'MakeId', 'Id', 'cylinders', 'horsepower']"
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_columns(sql):\n",
    "    parsed = sqlglot.parse_one(sql)\n",
    "    columns = [col.output_name for col in parsed.find_all(exp.Column)]\n",
    "    return columns\n",
    "\n",
    "find_columns(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_nested_query_tokens(tokens):\n",
    "    outer_query = []\n",
    "    active_nested_query = []\n",
    "    nested_queries = []\n",
    "    \n",
    "    is_in_nested = False\n",
    "    parenth_level = 0\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        \n",
    "        if str(token) == '(':\n",
    "            parenth_level += 1\n",
    "            \n",
    "        elif str(token) == ')':\n",
    "            parenth_level -= 1\n",
    "            \n",
    "            if parenth_level == 0 and is_in_nested:\n",
    "                nested_queries.append(active_nested_query)\n",
    "                active_nested_query = []\n",
    "                is_in_nested = False\n",
    "            \n",
    "        elif str(token).upper() == 'SELECT':\n",
    "            if parenth_level == 1 and i > 0 and str(tokens[i-1]) == '(':\n",
    "                is_in_nested = True\n",
    "                \n",
    "        if is_in_nested:\n",
    "            active_nested_query.append(token)\n",
    "        else:\n",
    "            outer_query.append(token)\n",
    "            \n",
    "        i += 1\n",
    "        \n",
    "    return outer_query, nested_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<DML 'SELECT' at 0x7F2B5CB61F00>,\n",
       "  <Wildcard '*' at 0x7F2B2AF54760>,\n",
       "  <Keyword 'FROM' at 0x7F2B61AFB220>,\n",
       "  <Name 'studen...' at 0x7F2B61AF9F00>,\n",
       "  <Keyword 'WHERE' at 0x7F2B61AF9D20>,\n",
       "  <Name 'studen...' at 0x7F2B61AF8B20>,\n",
       "  <Keyword 'NOT' at 0x7F2B61AFB700>,\n",
       "  <Keyword 'IN' at 0x7F2B61AFA3E0>,\n",
       "  <Punctuation '(' at 0x7F2B61AFAEC0>,\n",
       "  <Punctuation ')' at 0x7F2B61AFB580>],\n",
       " [<DML 'SELECT' at 0x7F2B61AFA920>,\n",
       "  <Name 'studen...' at 0x7F2B61AFA560>,\n",
       "  <Keyword 'FROM' at 0x7F2B61AFB1C0>,\n",
       "  <Name 'studen...' at 0x7F2B61AFA740>]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "statement = sqlparse.parse('SELECT * FROM student_course_registrations WHERE student_id NOT IN ( SELECT student_id FROM student_course_attendance )')[0]\n",
    "tokens = [token for token in statement.flatten() if str(token).strip() != '']\n",
    "split_nested_query_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_set_query_tokens(tokens):\n",
    "    subqueries = []\n",
    "    current_query = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if str(token).upper() in ['UNION', 'EXCEPT', 'INTERSECT']:\n",
    "            subqueries.append(current_query)\n",
    "            current_query = []\n",
    "        else:\n",
    "            current_query.append(token)\n",
    "    subqueries.append(current_query)\n",
    "    return subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<DML 'SELECT' at 0x7F2B2AF57A00>,\n",
       "  <Name 'studen...' at 0x7F2B2AF56DA0>,\n",
       "  <Keyword 'FROM' at 0x7F2B2AF57280>,\n",
       "  <Name 'studen...' at 0x7F2B5CF6F400>],\n",
       " [<DML 'SELECT' at 0x7F2B5CF6C880>,\n",
       "  <Name 'studen...' at 0x7F2B5CF6C1C0>,\n",
       "  <Keyword 'FROM' at 0x7F2B5CF6C0A0>,\n",
       "  <Name 'studen...' at 0x7F2B5CF6E620>]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "statement = sqlparse.parse('SELECT student_id FROM student_course_registrations UNION SELECT student_id FROM student_course_attendance')[0]\n",
    "tokens = [token for token in statement.flatten() if str(token).strip() != '']\n",
    "split_set_query_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_simple_query_tokens(tokens, db_id, tables_trans_dict, columns_trans_dict):\n",
    "    tables_trans_dict = tables_trans_dict[db_id]\n",
    "    columns_trans_dict = columns_trans_dict[db_id]\n",
    "        \n",
    "    query = ' '.join([str(token) for token in tokens])\n",
    "    aliasing_rev = {new.lower(): old for old, new in get_tables_aliasing(query)}\n",
    "    columns_names = find_columns(query.replace('( )', '1'))\n",
    "    tables_names = find_tables(query.replace('( )', '1'))\n",
    "    \n",
    "    for i in reversed(range(len(tokens))):\n",
    "        if str(tokens[i].ttype).startswith('Token.Literal.String'): # skip if token is string value\n",
    "            continue\n",
    "        \n",
    "        if str(tokens[i]) in columns_names:\n",
    "            table_name = str(tokens[i-2]) if i >= 2 and str(tokens[i-1]) == '.' else None\n",
    "            if table_name is not None:\n",
    "                table_name = aliasing_rev.get(table_name.lower(), table_name)\n",
    "            \n",
    "            if table_name is None:\n",
    "                possible_table_names = [table_name for table_name, table_trans in columns_trans_dict.items() if table_name in [x.lower() for x in tables_names] and str(tokens[i]).lower() in table_trans]\n",
    "                assert len(possible_table_names) > 0\n",
    "                if len(possible_table_names) > 1 and not( ' union ' not in query.lower() and ' except ' not in query.lower() and ' intersect ' not in query.lower()):\n",
    "                    print('+', query, '|', str(tokens[i]), f'({db_id})')\n",
    "                table_name = possible_table_names[0]\n",
    "            column_name_pl = columns_trans_dict[table_name.lower()][str(tokens[i]).lower()]['name_original']\n",
    "            tokens[i].value = column_name_pl\n",
    "            \n",
    "        elif str(tokens[i]) in tables_names:\n",
    "            table_name_pl = tables_trans_dict[str(tokens[i]).lower()]['name_original']\n",
    "            tokens[i].value = table_name_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_query_tokens(tokens, db_id, tables_trans_dict, columns_trans_dict):\n",
    "    outer_query, nested_queries = split_nested_query_tokens(tokens)\n",
    "    \n",
    "    set_queries = split_set_query_tokens(outer_query)\n",
    "    for query in set_queries:\n",
    "        translate_simple_query_tokens(query, db_id, tables_trans_dict, columns_trans_dict)\n",
    "    \n",
    "    for query in nested_queries:\n",
    "        translate_query_tokens(query, db_id, tables_trans_dict, columns_trans_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_query(query, db_id, tables_trans_dict, columns_trans_dict):\n",
    "    statement = sqlparse.parse(query)[0]\n",
    "    tokens = [token for token in statement.flatten() if str(token).strip() != '']\n",
    "    translate_query_tokens(tokens, db_id, tables_trans_dict, columns_trans_dict)\n",
    "    return str(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT student_id FROM Rejestracje_Kursów_Studenckich UNION SELECT student_id FROM Frekwencja_Studentów_Na_Zajęciach intersect SELECT student_id FROM Frekwencja_Studentów_Na_Zajęciach'"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q0 = \"SELECT student_id FROM student_course_registrations UNION SELECT student_id FROM student_course_attendance intersect SELECT student_id FROM student_course_attendance\"\n",
    "translate_query(\n",
    "    q0,\n",
    "    \"student_assessment\",\n",
    "    create_tables_translation_dict('../auxiliary/translated_schema/tables_names.json'),\n",
    "    create_columns_translation_dict('../auxiliary/translated_schema/columns_names.json'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_samples(samples, tables_translation_dict, columns_translation_dict):\n",
    "    for sample in samples:\n",
    "        query_pl = translate_query(sample['query'], sample['db_id'], tables_translation_dict, columns_translation_dict)\n",
    "        sample['query'] = query_pl\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "Expected table name but got <Token token_type: TokenType.NUMBER, text: 1, line: 1, col: 25, start: 24, end: 24, comments: []>. Line 1, Col: 25.\n  SELECT count ( * ) FROM \u001b[4m1\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[830], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m tables_translation_dict \u001b[39m=\u001b[39m create_tables_translation_dict(\u001b[39m'\u001b[39m\u001b[39m../auxiliary/translated_schema/tables_names.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m columns_translation_dict \u001b[39m=\u001b[39m create_columns_translation_dict(\u001b[39m'\u001b[39m\u001b[39m../auxiliary/translated_schema/columns_names.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m trans_samples \u001b[39m=\u001b[39m translate_samples(samples, tables_translation_dict, columns_translation_dict)\n",
      "Cell \u001b[0;32mIn[827], line 3\u001b[0m, in \u001b[0;36mtranslate_samples\u001b[0;34m(samples, tables_translation_dict, columns_translation_dict)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranslate_samples\u001b[39m(samples, tables_translation_dict, columns_translation_dict):\n\u001b[1;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m samples:\n\u001b[0;32m----> 3\u001b[0m         query_pl \u001b[39m=\u001b[39m translate_query(sample[\u001b[39m'\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m'\u001b[39;49m], sample[\u001b[39m'\u001b[39;49m\u001b[39mdb_id\u001b[39;49m\u001b[39m'\u001b[39;49m], tables_translation_dict, columns_translation_dict)\n\u001b[1;32m      4\u001b[0m         sample[\u001b[39m'\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m query_pl\n\u001b[1;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m samples\n",
      "Cell \u001b[0;32mIn[824], line 4\u001b[0m, in \u001b[0;36mtranslate_query\u001b[0;34m(query, db_id, tables_trans_dict, columns_trans_dict)\u001b[0m\n\u001b[1;32m      2\u001b[0m statement \u001b[39m=\u001b[39m sqlparse\u001b[39m.\u001b[39mparse(query)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m tokens \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m statement\u001b[39m.\u001b[39mflatten() \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(token)\u001b[39m.\u001b[39mstrip() \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m translate_query_tokens(tokens, db_id, tables_trans_dict, columns_trans_dict)\n\u001b[1;32m      5\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(statement)\n",
      "Cell \u001b[0;32mIn[826], line 6\u001b[0m, in \u001b[0;36mtranslate_query_tokens\u001b[0;34m(tokens, db_id, tables_trans_dict, columns_trans_dict)\u001b[0m\n\u001b[1;32m      4\u001b[0m set_queries \u001b[39m=\u001b[39m split_set_query_tokens(outer_query)\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m set_queries:\n\u001b[0;32m----> 6\u001b[0m     translate_simple_query_tokens(query, db_id, tables_trans_dict, columns_trans_dict)\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m nested_queries:\n\u001b[1;32m      9\u001b[0m     translate_query_tokens(query, db_id, tables_trans_dict, columns_trans_dict)\n",
      "Cell \u001b[0;32mIn[829], line 7\u001b[0m, in \u001b[0;36mtranslate_simple_query_tokens\u001b[0;34m(tokens, db_id, tables_trans_dict, columns_trans_dict)\u001b[0m\n\u001b[1;32m      5\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(token) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens])\n\u001b[1;32m      6\u001b[0m aliasing_rev \u001b[39m=\u001b[39m {new\u001b[39m.\u001b[39mlower(): old \u001b[39mfor\u001b[39;00m old, new \u001b[39min\u001b[39;00m get_tables_aliasing(query)}\n\u001b[0;32m----> 7\u001b[0m columns_names \u001b[39m=\u001b[39m find_columns(query\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39;49m\u001b[39m( )\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      8\u001b[0m tables_names \u001b[39m=\u001b[39m find_tables(query\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m( )\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(tokens))):\n",
      "Cell \u001b[0;32mIn[787], line 2\u001b[0m, in \u001b[0;36mfind_columns\u001b[0;34m(sql)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_columns\u001b[39m(sql):\n\u001b[0;32m----> 2\u001b[0m     parsed \u001b[39m=\u001b[39m sqlglot\u001b[39m.\u001b[39;49mparse_one(sql)\n\u001b[1;32m      3\u001b[0m     columns \u001b[39m=\u001b[39m [col\u001b[39m.\u001b[39moutput_name \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m parsed\u001b[39m.\u001b[39mfind_all(exp\u001b[39m.\u001b[39mColumn)]\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m columns\n",
      "File \u001b[0;32m~/anaconda3/envs/polish-spider/lib/python3.10/site-packages/sqlglot/__init__.py:125\u001b[0m, in \u001b[0;36mparse_one\u001b[0;34m(sql, read, dialect, into, **opts)\u001b[0m\n\u001b[1;32m    123\u001b[0m     result \u001b[39m=\u001b[39m dialect\u001b[39m.\u001b[39mparse_into(into, sql, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopts)\n\u001b[1;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[39m=\u001b[39m dialect\u001b[39m.\u001b[39;49mparse(sql, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopts)\n\u001b[1;32m    127\u001b[0m \u001b[39mfor\u001b[39;00m expression \u001b[39min\u001b[39;00m result:\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m expression:\n",
      "File \u001b[0;32m~/anaconda3/envs/polish-spider/lib/python3.10/site-packages/sqlglot/dialects/dialect.py:311\u001b[0m, in \u001b[0;36mDialect.parse\u001b[0;34m(self, sql, **opts)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m, sql: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopts) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mList[t\u001b[39m.\u001b[39mOptional[exp\u001b[39m.\u001b[39mExpression]]:\n\u001b[0;32m--> 311\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparser(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopts)\u001b[39m.\u001b[39;49mparse(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(sql), sql)\n",
      "File \u001b[0;32m~/anaconda3/envs/polish-spider/lib/python3.10/site-packages/sqlglot/parser.py:979\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self, raw_tokens, sql)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\n\u001b[1;32m    966\u001b[0m     \u001b[39mself\u001b[39m, raw_tokens: t\u001b[39m.\u001b[39mList[Token], sql: t\u001b[39m.\u001b[39mOptional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mList[t\u001b[39m.\u001b[39mOptional[exp\u001b[39m.\u001b[39mExpression]]:\n\u001b[1;32m    968\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[39m    Parses a list of tokens and returns a list of syntax trees, one tree\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[39m    per parsed SQL statement.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[39m        The list of the produced syntax trees.\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 979\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse(\n\u001b[1;32m    980\u001b[0m         parse_method\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_statement, raw_tokens\u001b[39m=\u001b[39;49mraw_tokens, sql\u001b[39m=\u001b[39;49msql\n\u001b[1;32m    981\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/polish-spider/lib/python3.10/site-packages/sqlglot/parser.py:1045\u001b[0m, in \u001b[0;36mParser._parse\u001b[0;34m(self, parse_method, raw_tokens, sql)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokens \u001b[39m=\u001b[39m tokens\n\u001b[1;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_advance()\n\u001b[0;32m-> 1045\u001b[0m expressions\u001b[39m.\u001b[39mappend(parse_method(\u001b[39mself\u001b[39;49m))\n\u001b[1;32m   1047\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokens):\n\u001b[1;32m   1048\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraise_error(\u001b[39m\"\u001b[39m\u001b[39mInvalid expression / Unexpected token\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/polish-spider/lib/python3.10/site-packages/sqlglot/parser.py:1235\u001b[0m, in \u001b[0;36mParser._parse_statement\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1232\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_command()\n\u001b[1;32m   1234\u001b[0m expression \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_expression()\n\u001b[0;32m-> 1235\u001b[0m expression \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_set_operations(expression) \u001b[39mif\u001b[39;00m expression \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_select()\n\u001b[1;32m   1236\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_query_modifiers(expression)\n",
      "File \u001b[0;32m~/anaconda3/envs/polish-spider/lib/python3.10/site-packages/sqlglot/parser.py:2145\u001b[0m, in \u001b[0;36mParser._parse_select\u001b[0;34m(self, nested, table, parse_subquery_alias)\u001b[0m\n\u001b[1;32m   2142\u001b[0m     this\u001b[39m.\u001b[39mset(\u001b[39m\"\u001b[39m\u001b[39minto\u001b[39m\u001b[39m\"\u001b[39m, into)\n\u001b[1;32m   2144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m from_:\n\u001b[0;32m-> 2145\u001b[0m     from_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_from()\n\u001b[1;32m   2147\u001b[0m \u001b[39mif\u001b[39;00m from_:\n\u001b[1;32m   2148\u001b[0m     this\u001b[39m.\u001b[39mset(\u001b[39m\"\u001b[39m\u001b[39mfrom\u001b[39m\u001b[39m\"\u001b[39m, from_)\n",
      "File \u001b[0;32m~/anaconda3/envs/polish-spider/lib/python3.10/site-packages/sqlglot/parser.py:2300\u001b[0m, in \u001b[0;36mParser._parse_from\u001b[0;34m(self, joins, skip_from_token)\u001b[0m\n\u001b[1;32m   2296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_from_token \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match(TokenType\u001b[39m.\u001b[39mFROM):\n\u001b[1;32m   2297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2299\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpression(\n\u001b[0;32m-> 2300\u001b[0m     exp\u001b[39m.\u001b[39mFrom, comments\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prev_comments, this\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_table(joins\u001b[39m=\u001b[39;49mjoins)\n\u001b[1;32m   2301\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/polish-spider/lib/python3.10/site-packages/sqlglot/parser.py:2641\u001b[0m, in \u001b[0;36mParser._parse_table\u001b[0;34m(self, schema, joins, alias_tokens, parse_bracket)\u001b[0m\n\u001b[1;32m   2638\u001b[0m bracket \u001b[39m=\u001b[39m parse_bracket \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_bracket(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   2639\u001b[0m bracket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpression(exp\u001b[39m.\u001b[39mTable, this\u001b[39m=\u001b[39mbracket) \u001b[39mif\u001b[39;00m bracket \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2640\u001b[0m this \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mcast(\n\u001b[0;32m-> 2641\u001b[0m     exp\u001b[39m.\u001b[39mExpression, bracket \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_bracket(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_table_parts(schema\u001b[39m=\u001b[39;49mschema))\n\u001b[1;32m   2642\u001b[0m )\n\u001b[1;32m   2644\u001b[0m \u001b[39mif\u001b[39;00m schema:\n\u001b[1;32m   2645\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_schema(this\u001b[39m=\u001b[39mthis)\n",
      "File \u001b[0;32m~/anaconda3/envs/polish-spider/lib/python3.10/site-packages/sqlglot/parser.py:2607\u001b[0m, in \u001b[0;36mParser._parse_table_parts\u001b[0;34m(self, schema)\u001b[0m\n\u001b[1;32m   2604\u001b[0m         table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_table_part(schema\u001b[39m=\u001b[39mschema)\n\u001b[1;32m   2606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m table:\n\u001b[0;32m-> 2607\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraise_error(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mExpected table name but got \u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_curr\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpression(\n\u001b[1;32m   2610\u001b[0m     exp\u001b[39m.\u001b[39mTable, this\u001b[39m=\u001b[39mtable, db\u001b[39m=\u001b[39mdb, catalog\u001b[39m=\u001b[39mcatalog, pivots\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_pivots()\n\u001b[1;32m   2611\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/polish-spider/lib/python3.10/site-packages/sqlglot/parser.py:1089\u001b[0m, in \u001b[0;36mParser.raise_error\u001b[0;34m(self, message, token)\u001b[0m\n\u001b[1;32m   1077\u001b[0m error \u001b[39m=\u001b[39m ParseError\u001b[39m.\u001b[39mnew(\n\u001b[1;32m   1078\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m. Line \u001b[39m\u001b[39m{\u001b[39;00mtoken\u001b[39m.\u001b[39mline\u001b[39m}\u001b[39;00m\u001b[39m, Col: \u001b[39m\u001b[39m{\u001b[39;00mtoken\u001b[39m.\u001b[39mcol\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1079\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{\u001b[39;00mstart_context\u001b[39m}\u001b[39;00m\u001b[39m\\033\u001b[39;00m\u001b[39m[4m\u001b[39m\u001b[39m{\u001b[39;00mhighlight\u001b[39m}\u001b[39;00m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m{\u001b[39;00mend_context\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     end_context\u001b[39m=\u001b[39mend_context,\n\u001b[1;32m   1086\u001b[0m )\n\u001b[1;32m   1088\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_level \u001b[39m==\u001b[39m ErrorLevel\u001b[39m.\u001b[39mIMMEDIATE:\n\u001b[0;32m-> 1089\u001b[0m     \u001b[39mraise\u001b[39;00m error\n\u001b[1;32m   1091\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mappend(error)\n",
      "\u001b[0;31mParseError\u001b[0m: Expected table name but got <Token token_type: TokenType.NUMBER, text: 1, line: 1, col: 25, start: 24, end: 24, comments: []>. Line 1, Col: 25.\n  SELECT count ( * ) FROM \u001b[4m1\u001b[0m"
     ]
    }
   ],
   "source": [
    "with open('../auxiliary/translated_likes/train_spider.json') as f:\n",
    "    samples = json.load(f)\n",
    "    \n",
    "tables_translation_dict = create_tables_translation_dict('../auxiliary/translated_schema/tables_names.json')\n",
    "columns_translation_dict = create_columns_translation_dict('../auxiliary/translated_schema/columns_names.json')\n",
    "\n",
    "trans_samples = translate_samples(samples, tables_translation_dict, columns_translation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2440"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................'\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../auxiliary/translated_likes/train_spider.json', 'w') as f:\n",
    "        json.dump(trans_samples, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_complete(src_name, dst_name):\n",
    "    src_path = Path('../auxiliary') / src_name\n",
    "    dst_path = Path('../complete') / dst_name\n",
    "    \n",
    "    dst_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    for file in ['dev.json', 'train_others.json', 'train_spider.json']:\n",
    "        add_calculated_attributes_to_samples(\n",
    "            src_path / file,\n",
    "            dst_path / file,\n",
    "            src_path / 'tables.json'\n",
    "        )\n",
    "\n",
    "    create_gold_sql(\n",
    "        [\n",
    "            src_path / 'dev.json'\n",
    "        ],\n",
    "        dst_path / 'dev_gold.sql'\n",
    "    )\n",
    "\n",
    "    create_gold_sql(\n",
    "        [\n",
    "            src_path / 'train_spider.json',\n",
    "            src_path / 'train_others.json'\n",
    "        ],\n",
    "        dst_path / 'train_gold.sql'\n",
    "    )\n",
    "    \n",
    "    shutil.copyfile(src_path / 'tables.json', dst_path / 'tables.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_complete('machine_translated', 'machine_translated')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polish-spider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
